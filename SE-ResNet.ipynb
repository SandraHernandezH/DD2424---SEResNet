{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "!kill $(ps aux | grep './ngrok' | awk '{print $2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from ResNet50 import *\n",
    "from LoadDataset import *\n",
    "from tensorboard import notebook\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2)\n",
    "tf.print(a)\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "Loading ./Datasets/cifar-10-batches-py/data_batch_1 : 10000.\n",
      "Loading ./Datasets/cifar-10-batches-py/data_batch_2 : 10000.\n",
      "Loading ./Datasets/cifar-10-batches-py/data_batch_3 : 10000.\n",
      "Loading ./Datasets/cifar-10-batches-py/data_batch_4 : 10000.\n",
      "Loading ./Datasets/cifar-10-batches-py/data_batch_5 : 10000.\n",
      "Loading ./Datasets/cifar-10-batches-py/test_batch : 10000.\n",
      "Train data: (50000, 32, 32, 3) (50000,)\n",
      "Test data : (10000, 32, 32, 3) (10000,)\n",
      "normalizing data...\n",
      "augmenting data...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Reshape\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow.keras.backend as K\n",
    "from Squeeze_and_Excite import Squeeze_and_Excite \n",
    "from ResNet50 import *\n",
    "from LoadDataset import *\n",
    "\n",
    "#%load_ext tensorboard\n",
    "#!rm -rf ./logs/\n",
    "\n",
    "epoch = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "dataset = 10\n",
    "momentum = 0.9\n",
    "wd = 1e-2\n",
    "\n",
    "#lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.1, decay_steps=100000, decay_rate=0.9)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=momentum)\n",
    "#optimizer = tfa.optimizers.SGDW(learning_rate=lr_schedule, weight_decay=wd, momentum=momentum)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, decay=decay_rate, momentum=momentum)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer_sgd = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "optimizer_adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "optimizer = optimizer_adam\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "test_top1 = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name='test_1_accuracy') \n",
    "test_top5 = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='test_5_accuracy') \n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "        K.set_learning_phase(1)\n",
    "        predictions = model(images, training = True)\n",
    "        loss =loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "        \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    K.set_learning_phase(0)\n",
    "    predictions = model(images, training = False)\n",
    "    t_loss =loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    test_top1(labels, predictions)\n",
    "    test_top5(labels, predictions)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    save_model_dir = \"./checkpoints_norm\"\n",
    "\n",
    "    print(\"loading data ...\")\n",
    "    train_X, train_lab, test_X, test_lab = get_data(dataset)\n",
    "    print(\"normalizing data...\")\n",
    "    train_X, test_X = normalize(train_X, test_X)\n",
    "    print(\"augmenting data...\")\n",
    "    train_X = data_augmentation(train_X)\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_X, train_lab)).batch(batch_size)\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_X, test_lab)).batch(batch_size)\n",
    "    \n",
    "\n",
    "    #model = ResNet50(include_top=True, weights=None, squeeze=False, squeeze_type='Normal')#, input_tensor = tf.data.Dataset.from_tensor_slices((train_X)))\n",
    "    #input_tensor = tf.placeholder(tf.float32, shape = [None, train_X.shape[0], train_X.shape[1], train_X.shape[2]])\n",
    "    model = ResNet50(include_top = True, squeeze = True, squeeze_type = 'normal', classes = dataset) #pre, identity, normal\n",
    "\n",
    "    \"\"\"\n",
    "    features, label = iter(train_dataset).next()\n",
    "    print(\"example features:\", features[0])\n",
    "    print(\"example label:\", label[0])\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint_dir = os.path.join(save_model_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n",
    "    #checkpoint.restore(manager.latest_checkpoint)\n",
    "    #if manager.latest_checkpoint:\n",
    "    #    print(\"Restaurado de {}\".format(manager.latest_checkpoint))\n",
    "    #else:\n",
    "    #    print(\"Inicializando desde cero\")\n",
    "\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "    test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-13 08:16:19--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
      "Resolving bin.equinox.io (bin.equinox.io)... 54.209.108.221, 52.54.124.219, 18.213.76.145, ...\n",
      "Connecting to bin.equinox.io (bin.equinox.io)|54.209.108.221|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13773305 (13M) [application/octet-stream]\n",
      "Saving to: ‘ngrok-stable-linux-amd64.zip.2’\n",
      "\n",
      "ngrok-stable-linux- 100%[===================>]  13.13M  15.2MB/s    in 0.9s    \n",
      "\n",
      "2020-05-13 08:16:20 (15.2 MB/s) - ‘ngrok-stable-linux-amd64.zip.2’ saved [13773305/13773305]\n",
      "\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "  inflating: ngrok                   \n"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip -o ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "logdir = \"logs\"\n",
    "LOG_DIR = logdir\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 39233.15234375, Train Accuracy: 9.977999687194824, Test Loss: 2.3096718788146973, Test Accuracy: 10.0\n",
      "Top1 Error: 90.0, Top5 Error: 50.0\n",
      "Epoch 2, Train Loss: 2.305590867996216, Train Accuracy: 9.920000076293945, Test Loss: 2.305086374282837, Test Accuracy: 10.0\n",
      "Top1 Error: 90.0, Top5 Error: 50.0\n",
      "Epoch 3, Train Loss: 2.304503917694092, Train Accuracy: 9.878000259399414, Test Loss: 2.306164264678955, Test Accuracy: 10.0\n",
      "Top1 Error: 90.0, Top5 Error: 50.0\n",
      "Epoch 4, Train Loss: 2.304453134536743, Train Accuracy: 9.842000007629395, Test Loss: 2.3050594329833984, Test Accuracy: 10.0\n",
      "Top1 Error: 90.0, Top5 Error: 50.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-553be6c8945b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#optimizer = optimizer_sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        #print(i)\n",
    "        #if i == 1:\n",
    "            #optimizer = optimizer_sgd\n",
    "        for images, labels in train_data:\n",
    "            train_step(images, labels)\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=i+1)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=i+1)\n",
    "\n",
    "            \n",
    "        for images, labels in test_data:\n",
    "            test_step(images, labels)\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=i+1)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=i+1)\n",
    "            tf.summary.scalar('top1error', (1 - test_top1.result())*100, step=i+1)\n",
    "            tf.summary.scalar('top5error', (1 - test_top5.result())*100, step=i+1)\n",
    "            \n",
    "\n",
    "        template = 'Epoch {}, Train Loss: {}, Train Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print(template.format(i+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "\n",
    "        template = 'Top1 Error: {}, Top5 Error: {}'\n",
    "        print(template.format((1 - test_top1.result())*100,\\\n",
    "                            (1 - test_top5.result())*100))\n",
    "\n",
    "        save_path = manager.save()\n",
    "\n",
    "        train_losses.append(train_loss.result())\n",
    "        train_accs.append(train_accuracy.result())\n",
    "        test_losses.append(test_loss.result())\n",
    "        test_accs.append(test_accuracy.result())\n",
    "\n",
    "        # Reinicia las metricas para el siguiente epoch.\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "\n",
    "    t = np.linspace(1, epoch, num=epoch)\n",
    "    plot1 = plt.figure(1)\n",
    "    train_l = plt.plot(t, train_losses, 'b', label='training')\n",
    "    test_l = plt.plot(t, test_losses, 'r', label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(handles=[train_l, test_l])\n",
    "    plt.title('Loss evolution')\n",
    "    plt.savefig('./Result_Pics/loss_train')\n",
    "    plot2 = plt.figure(2)\n",
    "    train_a = plt.plot(t, train_accs, 'b', label='training')\n",
    "    test_a = plt.plot(t, test_accs, 'r', label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(handles=[train_l, test_l])\n",
    "    plt.title('Accuracy evolution')\n",
    "    plt.savefig('.//Result_Pics//acc_train')\n",
    "    plt.show()\n",
    "    \n",
    "    a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
