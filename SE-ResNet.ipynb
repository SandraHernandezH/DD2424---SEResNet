{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from ResNet50 import *\n",
    "from LoadDataset import *\n",
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2)\n",
    "tf.print(a)\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data 2...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-85775ff707db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading data 2...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_lab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"normalizing data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DD2424---SEResNet/LoadDataset.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mimg_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./Datasets/cifar-100-python'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DD2424---SEResNet/LoadDataset.py\u001b[0m in \u001b[0;36mload_data100\u001b[0;34m(files, data_dir, label_count)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle100\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mdata_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_n\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0munpickle100\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Reshape\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow.keras.backend as K\n",
    "from Squeeze_and_Excite import Squeeze_and_Excite \n",
    "from ResNet50 import *\n",
    "from LoadDataset import *\n",
    "\n",
    "#%load_ext tensorboard\n",
    "#!rm -rf ./logs/\n",
    "\n",
    "epoch = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "dataset = 100\n",
    "\n",
    "\n",
    "def oneHotEncoder(data):\n",
    "    label_encoder = LabelEncoder()\n",
    "    values = np.array(data)\n",
    "    onehot_encoder = OneHotEncoder(sparse = False)\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return onehot_encoded\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "test_top1 = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name='test_1_accuracy') \n",
    "test_top5 = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='test_5_accuracy') \n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training = True)\n",
    "        loss =loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "        \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training = False)\n",
    "    t_loss =loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    test_top1(labels, predictions)\n",
    "    test_top5(labels, predictions)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    save_model_dir = \"./checkpoints_pre\"\n",
    "\n",
    "    print(\"loading data 2...\")\n",
    "    train_X, train_lab, test_X, test_lab = get_data(dataset)\n",
    "    print(\"normalizing data...\")\n",
    "    train_X, test_X = normalize(train_X, test_X)\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_X, train_lab)).batch(batch_size)\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_X, test_lab)).batch(batch_size)\n",
    "\n",
    "    #model = ResNet50(include_top=True, weights=None, squeeze=False, squeeze_type='Normal')#, input_tensor = tf.data.Dataset.from_tensor_slices((train_X)))\n",
    "    #input_tensor = tf.placeholder(tf.float32, shape = [None, train_X.shape[0], train_X.shape[1], train_X.shape[2]])\n",
    "    model = ResNet50(include_top=True, squeeze=False, squeeze_type='normal', classes=dataset) #pre, identity, normal\n",
    "\n",
    "    \"\"\"\n",
    "    features, label = iter(train_dataset).next()\n",
    "    print(\"example features:\", features[0])\n",
    "    print(\"example label:\", label[0])\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint_dir = os.path.join(save_model_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n",
    "    checkpoint.restore(manager.latest_checkpoint)\n",
    "    #if manager.latest_checkpoint:\n",
    "    #    print(\"Restaurado de {}\".format(manager.latest_checkpoint))\n",
    "    #else:\n",
    "    #    print(\"Inicializando desde cero\")\n",
    "\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "    test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1897659de43e3323\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1897659de43e3323\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir train_log_dir (started 0:00:00 ago; pid 7867)\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir train_log_dir\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control TensorBoard display. If no port is provided, \n",
    "# the most recently launched TensorBoard is used\n",
    "notebook.display(port=6006, height=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        print(i)\n",
    "        for images, labels in train_data:\n",
    "            train_step(images, labels)\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=i+1)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=i+1)\n",
    "\n",
    "            \n",
    "        for images, labels in test_data:\n",
    "            test_step(images, labels)\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=i+1)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=i+1)\n",
    "\n",
    "        template = 'Epoch {}, Train Loss: {}, Train Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print(template.format(i+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "\n",
    "        template = 'Top1 Error: {}, Top5 Error: {}'\n",
    "        print(template.format((1 - test_top1.result())*100,\\\n",
    "                            (1 - test_top5.result())*100))\n",
    "\n",
    "        save_path = manager.save()\n",
    "\n",
    "        train_losses.append(train_loss.result())\n",
    "        train_accs.append(train_accuracy.result())\n",
    "        test_losses.append(test_loss.result())\n",
    "        test_accs.append(test_accuracy.result())\n",
    "\n",
    "        # Reinicia las metricas para el siguiente epoch.\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "\n",
    "    t = np.linspace(1, epoch, num=epoch)\n",
    "    plot1 = plt.figure(1)\n",
    "    plt.plot(t, train_losses, 'b')\n",
    "    plt.plot(t, test_losses, 'r')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training loss evolution')\n",
    "    plt.savefig('./Result_Pics/loss_train')\n",
    "    plot2 = plt.figure(2)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(t, train_accs, 'b')\n",
    "    plt.plot(t, test_accs, 'r')\n",
    "    plt.title('Training accuracy evolution')\n",
    "    plt.savefig('.//Result_Pics//acc_train')\n",
    "    plt.show()\n",
    "    \n",
    "    a = 1"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
